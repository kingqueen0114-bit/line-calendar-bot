"""
Agent Optimizer using Agent Lightning
LINE Calendar Bot ã®å¿œç­”ã‚’å¼·åŒ–å­¦ç¿’ã§æœ€é©åŒ–
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable
import agentlightning as agl

from config import AgentLightningConfig, PROMPT_TEMPLATES


class LineCalendarAgent:
    """
    LINE Calendar Bot ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    Agent Lightning ã§æœ€é©åŒ–å¯èƒ½ãªãƒ©ãƒƒãƒ‘ãƒ¼
    """

    def __init__(self, config: Optional[AgentLightningConfig] = None):
        self.config = config or AgentLightningConfig.from_env()
        self.system_prompt = PROMPT_TEMPLATES["system"]
        self.optimized_prompts: Dict[str, str] = {}
        self._load_optimized_prompts()

    def _load_optimized_prompts(self):
        """æœ€é©åŒ–æ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        prompt_file = Path(self.config.data_dir) / "optimized_prompts.json"
        if prompt_file.exists():
            with open(prompt_file, "r", encoding="utf-8") as f:
                self.optimized_prompts = json.load(f)
            print(f"Loaded {len(self.optimized_prompts)} optimized prompts")

    def _save_optimized_prompts(self):
        """æœ€é©åŒ–æ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜"""
        prompt_file = Path(self.config.data_dir) / "optimized_prompts.json"
        prompt_file.parent.mkdir(parents=True, exist_ok=True)
        with open(prompt_file, "w", encoding="utf-8") as f:
            json.dump(self.optimized_prompts, f, ensure_ascii=False, indent=2)

    def get_system_prompt(self, task_type: Optional[str] = None) -> str:
        """
        ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
        æœ€é©åŒ–æ¸ˆã¿ã®ã‚‚ã®ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨

        Args:
            task_type: ã‚¿ã‚¹ã‚¯ã®ç¨®é¡

        Returns:
            ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        if task_type and task_type in self.optimized_prompts:
            return self.optimized_prompts[task_type]
        return self.system_prompt

    def update_optimized_prompt(self, task_type: str, prompt: str):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›´æ–°"""
        self.optimized_prompts[task_type] = prompt
        self._save_optimized_prompts()


class AgentOptimizer:
    """
    Agent Lightning ã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæœ€é©åŒ–
    """

    def __init__(self, config: Optional[AgentLightningConfig] = None):
        self.config = config or AgentLightningConfig.from_env()
        self.agent = LineCalendarAgent(config)
        self.training_history: List[Dict[str, Any]] = []

    def prepare_training_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Agent Lightningç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Args:
            data: collector.get_training_data() ã®å‡ºåŠ›

        Returns:
            Agent Lightningç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        formatted_data = []

        for item in data:
            formatted_data.append({
                "messages": [
                    {"role": "system", "content": self.agent.get_system_prompt(item.get("task_type"))},
                    {"role": "user", "content": item["input"]},
                    {"role": "assistant", "content": item["output"]},
                ],
                "reward": item.get("reward", 0.0),
                "task_type": item.get("task_type", "general"),
            })

        return formatted_data

    def create_reward_function(self) -> Callable[[str, str, Dict[str, Any]], float]:
        """
        å ±é…¬é–¢æ•°ã‚’ä½œæˆ

        Returns:
            å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
        """
        def reward_fn(user_input: str, bot_output: str, context: Dict[str, Any]) -> float:
            """
            å¿œç­”ã®å“è³ªã«åŸºã¥ã„ã¦å ±é…¬ã‚’è¨ˆç®—

            è©•ä¾¡åŸºæº–:
            - æ—¥æœ¬èªã®è‡ªç„¶ã•
            - ã‚¿ã‚¹ã‚¯å®Œäº†åº¦
            - å¿œç­”ã®ç°¡æ½”ã•
            - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            """
            reward = 0.0

            # åŸºæœ¬çš„ãªå¿œç­”ãƒã‚§ãƒƒã‚¯
            if not bot_output or len(bot_output.strip()) == 0:
                return self.config.failure_reward

            # æˆåŠŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            success_keywords = ["å®Œäº†", "ç™»éŒ²", "ä½œæˆ", "è¨­å®š", "è¿½åŠ ", "å‰Šé™¤"]
            if any(kw in bot_output for kw in success_keywords):
                reward += 0.3

            # ã‚¨ãƒ©ãƒ¼å¿œç­”ãƒã‚§ãƒƒã‚¯
            error_keywords = ["ã‚¨ãƒ©ãƒ¼", "å¤±æ•—", "ã§ãã¾ã›ã‚“", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"]
            if any(kw in bot_output for kw in error_keywords):
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚é©åˆ‡ã«èª¬æ˜ã—ã¦ã„ã‚Œã°éƒ¨åˆ†ç‚¹
                if len(bot_output) > 20:
                    reward += 0.1
                else:
                    reward -= 0.2

            # å¿œç­”ã®é•·ã•ãƒã‚§ãƒƒã‚¯ï¼ˆé©åˆ‡ãªé•·ã•ã‚’è©•ä¾¡ï¼‰
            output_len = len(bot_output)
            if 10 <= output_len <= 200:
                reward += 0.2
            elif output_len > 500:
                reward -= 0.1  # é•·ã™ãã‚‹å¿œç­”

            # çµµæ–‡å­—ä½¿ç”¨ï¼ˆè¦ªã—ã¿ã‚„ã™ã•ï¼‰
            emoji_chars = ["âœ…", "ğŸ“…", "â°", "ğŸ“", "ğŸ””", "ğŸ‘"]
            if any(e in bot_output for e in emoji_chars):
                reward += 0.1

            # æ­£è¦åŒ–
            reward = max(min(reward, self.config.success_reward), self.config.failure_reward)

            return reward

        return reward_fn

    def run_optimization(
        self,
        training_data: List[Dict[str, Any]],
        num_iterations: int = 100,
        callback: Optional[Callable[[int, float], None]] = None,
    ) -> Dict[str, Any]:
        """
        å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’å®Ÿè¡Œ

        Args:
            training_data: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
            num_iterations: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
            callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (iteration, reward) -> None

        Returns:
            æœ€é©åŒ–çµæœ
        """
        print(f"Starting optimization with {len(training_data)} samples...")

        formatted_data = self.prepare_training_data(training_data)
        reward_fn = self.create_reward_function()

        results = {
            "start_time": datetime.now().isoformat(),
            "num_samples": len(training_data),
            "num_iterations": num_iterations,
            "rewards": [],
            "best_reward": float("-inf"),
            "final_prompts": {},
        }

        try:
            # Agent Lightning ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ã‚’åˆæœŸåŒ–
            with agl.Tracer(
                name="line_calendar_bot_optimization",
                metadata={
                    "model": self.config.model_name,
                    "samples": len(training_data),
                }
            ) as tracer:
                for iteration in range(num_iterations):
                    total_reward = 0.0

                    for item in formatted_data:
                        # ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç™ºè¡Œ
                        agl.emit_step(
                            name=f"train_{item.get('task_type', 'general')}",
                            input=item["messages"][1]["content"],
                            output=item["messages"][2]["content"],
                        )

                        # å ±é…¬ã‚’è¨ˆç®—ã—ã¦ç™ºè¡Œ
                        reward = reward_fn(
                            item["messages"][1]["content"],
                            item["messages"][2]["content"],
                            {"task_type": item.get("task_type")},
                        )
                        agl.emit_reward(reward=reward)
                        total_reward += reward

                    avg_reward = total_reward / len(formatted_data) if formatted_data else 0
                    results["rewards"].append(avg_reward)

                    if avg_reward > results["best_reward"]:
                        results["best_reward"] = avg_reward

                    if callback:
                        callback(iteration, avg_reward)

                    if (iteration + 1) % 10 == 0:
                        print(f"Iteration {iteration + 1}/{num_iterations}, Avg Reward: {avg_reward:.4f}")

        except Exception as e:
            print(f"Warning: Agent Lightning optimization error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªçµ±è¨ˆã®ã¿è¨ˆç®—
            for item in formatted_data:
                reward = reward_fn(
                    item["messages"][1]["content"],
                    item["messages"][2]["content"],
                    {"task_type": item.get("task_type")},
                )
                results["rewards"].append(reward)

            if results["rewards"]:
                results["best_reward"] = max(results["rewards"])

        results["end_time"] = datetime.now().isoformat()
        results["avg_final_reward"] = sum(results["rewards"][-10:]) / min(10, len(results["rewards"])) if results["rewards"] else 0

        # çµæœã‚’ä¿å­˜
        self._save_results(results)

        return results

    def _save_results(self, results: Dict[str, Any]):
        """æœ€é©åŒ–çµæœã‚’ä¿å­˜"""
        results_dir = Path(self.config.data_dir) / "optimization_results"
        results_dir.mkdir(parents=True, exist_ok=True)

        filename = f"result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_dir / filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"Results saved to {results_dir / filename}")

    def get_optimization_history(self) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–å±¥æ­´ã‚’å–å¾—"""
        results_dir = Path(self.config.data_dir) / "optimization_results"
        if not results_dir.exists():
            return []

        history = []
        for result_file in sorted(results_dir.glob("result_*.json")):
            with open(result_file, "r", encoding="utf-8") as f:
                history.append(json.load(f))

        return history


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    from collector import DataCollector

    collector = DataCollector()

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    test_data = [
        {
            "user_message": "æ˜æ—¥ã®åˆå¾Œ3æ™‚ã«ä¼šè­°ã‚’å…¥ã‚Œã¦",
            "bot_response": "âœ… æ˜æ—¥ã®15:00ã«ã€Œä¼šè­°ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚",
            "task_type": "calendar_create",
        },
        {
            "user_message": "ä»Šé€±ã®äºˆå®šã‚’æ•™ãˆã¦",
            "bot_response": "ğŸ“… ä»Šé€±ã®äºˆå®šã§ã™:\n- 2/7 15:00 ä¼šè­°\n- 2/8 10:00 æ­¯åŒ»è€…",
            "task_type": "calendar_query",
        },
        {
            "user_message": "è²·ã„ç‰©ãƒªã‚¹ãƒˆã«ç‰›ä¹³ã‚’è¿½åŠ ",
            "bot_response": "âœ… ã‚¿ã‚¹ã‚¯ã€Œç‰›ä¹³ã‚’è²·ã†ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚",
            "task_type": "task_create",
        },
    ]

    for item in test_data:
        collector.record_interaction(
            user_id="test_user",
            task_type=item["task_type"],
            user_message=item["user_message"],
            bot_response=item["bot_response"],
        )

    # æœ€é©åŒ–ã‚’å®Ÿè¡Œ
    optimizer = AgentOptimizer()
    training_data = collector.get_training_data()

    results = optimizer.run_optimization(
        training_data,
        num_iterations=10,
        callback=lambda i, r: print(f"  Progress: {i+1}/10, Reward: {r:.4f}"),
    )

    print("\n=== Optimization Results ===")
    print(f"Best Reward: {results['best_reward']:.4f}")
    print(f"Final Avg Reward: {results['avg_final_reward']:.4f}")
